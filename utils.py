import random
import torch
from nltk.translate.bleu_score import sentence_bleu
from rouge import Rouge
import torch.nn.functional as F
from tokenizers_classes	 import CharacterLevelTokenizer, SubwordTokenizer


def evaluate_bleu_and_rouge(model, tokenizer, device, dev_samples, vocab_size, context_size=5, max_length=1024, num_samples=200):
    """
    Evaluates BLEU and ROUGE scores for the model.

    Args:
        model: The trained model.
        tokenizer: The tokenizer used for encoding/decoding.
        device: The device ('cpu' or 'cuda').
        dev_samples: List of samples from the dev set.
        context_size: Number of tokens to use for the context (starting tokens).
        max_length: Maximum length of the generated sequence.
        num_samples: The number of samples to evaluate (subsampled).

    Returns:
        BLEU score and ROUGE score for the generated sequences.
    """
    # For BLEU and ROUGE calculation
    bleu_scores = []
    rouge_scores = []

    rouge = Rouge()

    # Subsample num_samples samples from the dev set
    sampled_dev_set = random.sample(dev_samples, num_samples)

    for sample in sampled_dev_set:
        # Select the first 'context_size' tokens as the context
        sample = tokenizer.tokenize(sample)
        context = sample[:context_size]
        initial_string = tokenizer.detokenize(context)

        # Generate the sequence using the model
        generated_sequence = generate_sequence(model, device, tokenizer, initial_string, vocab_size, max_length)

        # The remaining part of the sample is the ground truth (completion)
        ground_truth = sample[context_size:]

        bleu_score = sentence_bleu(ground_truth, generated_sequence)
        bleu_scores.append(bleu_score)

        # Calculate ROUGE score for the generated sequence
        rouge_score = rouge.get_scores(''.join(generated_sequence), ''.join(ground_truth))
        rouge_scores.append(rouge_score[0]['rouge-l']['f'])

    # Calculate average BLEU and ROUGE scores
    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)
    avg_rouge_score = sum(rouge_scores) / len(rouge_scores)

    print(f'Average BLEU score: {avg_bleu_score:.4f}')
    print(f'Average ROUGE-L score: {avg_rouge_score:.4f}')

    return avg_bleu_score, avg_rouge_score


def generate_sequence(model, device, tokenizer, context, vocab_size, max_length=256, context_size=256, top_k=10):
    """
    Generates a sequence using the provided model and tokenizer with the given context.

    Args:
        model: The trained transformer model.
        tokenizer: The tokenizer used to encode/decode tokens.
        context: The starting sequence to generate from (typically a list with the <SOS> token).
        vocab_size: Size of the vocabulary.
        max_length: The maximum length for the generated sequence.
        context_size: The maximum number of tokens the model can attend to.
        top_k: The number of top tokens to consider during sampling (for top-k sampling).

    Returns:
        generated_sequence: List of tokens generated by the model.
    """
    # Prepare the context (input sequence)
    context_tokens = tokenizer.tokenize(context)

    if type(tokenizer) == CharacterLevelTokenizer:
        context_ids = torch.tensor([tokenizer.vocab[token] for token in context_tokens]).unsqueeze(0).to(device)
    elif type(tokenizer) == SubwordTokenizer:
        context_ids = torch.tensor(context_tokens).unsqueeze(0).to(device)

    generated_sequence = context_tokens[:-1]
    print(generated_sequence)

    # Start generating tokens
    for _ in range(max_length):
        # print("Start a new token")
        # Get the logits for the next token from the model
        logits = model(context_ids)

        # Focus on the last token's logits (output of the transformer)
        logits = logits[:, -1, :]  # Shape: (batch_size, vocab_size)

        # Apply top-k sampling if top_k is set
        if top_k > 0:
            # Get the top-k token logits
            top_k_values, top_k_indices = torch.topk(logits, top_k)
            # Normalize probabilities
            top_k_probs = F.softmax(top_k_values, dim=-1)

            # Detach from the computation graph before using numpy()
            top_k_indices = top_k_indices.detach().cpu().numpy()[0]
            top_k_probs = top_k_probs.detach().cpu().numpy()[0]

            # Sample a token from the top-k candidates
            # print("Choices:")
            # print(top_k_indices)
            next_token_id = random.choices(top_k_indices, top_k_probs)[0]
        else:
            # Otherwise use argmax (greedy sampling)
            next_token_id = torch.argmax(logits, dim=-1).item()

        # Get the next token (from ID to token)
        # print(tokenizer.vocab)
        # print("Next token: ")
        # print(next_token_id)
        # print()

        if type(tokenizer) == CharacterLevelTokenizer:
            next_token = list(tokenizer.vocab.keys())[list(tokenizer.vocab.values()).index(next_token_id)]
        elif type(tokenizer) == SubwordTokenizer:
            next_token = next_token_id

        # Add the generated token to the context sequence
        generated_sequence.append(next_token)

        # Append the next token to the context
        context_ids = torch.cat((context_ids, torch.tensor([[next_token_id]]).to(device)), dim=1)

        # Trim the context to the maximum context size
        if context_ids.size(1) > context_size:
            context_ids = context_ids[:, 1:]

        # Stop if <EOS> token is generated
        if next_token == tokenizer.end_token:
            break

    print(generated_sequence)
    generated_sequence = tokenizer.detokenize(generated_sequence)
    # print(generated_sequence)
    return generated_sequence
