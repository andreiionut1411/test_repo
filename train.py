from decepticon import causal_language_model_loss, DecoderOnlyTransformer
from dataset_processor import ShakespeareDatasetProcessor
from tokenizers_classes import CharacterLevelTokenizer, SubwordTokenizer
from torch.utils.data import Dataset, DataLoader
import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
import random
from transformers import get_linear_schedule_with_warmup
from nltk.translate.bleu_score import sentence_bleu
from rouge import Rouge
from tqdm import tqdm
from torch.nn.utils.rnn import pad_sequence


def evaluate_bleu_and_rouge(model, tokenizer, device, dev_samples, vocab_size, context_size=10, max_length=1024, num_samples=50):
    """
    Evaluates BLEU and ROUGE scores for the model.

    Args:
        model: The trained model.
        tokenizer: The tokenizer used for encoding/decoding.
        device: The device ('cpu' or 'cuda').
        dev_samples: List of samples from the dev set.
        context_size: Number of tokens to use for the context (starting tokens).
        max_length: Maximum length of the generated sequence.
        num_samples: The number of samples to evaluate (subsampled).

    Returns:
        BLEU score and ROUGE score for the generated sequences.
    """
    # For BLEU and ROUGE calculation
    bleu_scores = []
    rouge_scores = []

    rouge = Rouge()

    # Subsample num_samples samples from the dev set
    sampled_dev_set = random.sample(dev_samples, num_samples)

    for sample in sampled_dev_set:
        # Select the first 'context_size' tokens as the context
        sample = tokenizer.tokenize(sample)
        context = sample[:context_size]
        initial_string = tokenizer.detokenize(context)

        # Generate the sequence using the model
        generated_sequence = generate_sequence(model, device, tokenizer, initial_string, vocab_size, max_length)

        # The remaining part of the sample is the ground truth (completion)
        ground_truth = sample[context_size:]

        bleu_score = sentence_bleu(ground_truth, generated_sequence)
        bleu_scores.append(bleu_score)

        # Calculate ROUGE score for the generated sequence
        rouge_score = rouge.get_scores(''.join(generated_sequence), ''.join(ground_truth))
        rouge_scores.append(rouge_score[0]['rouge-l']['f'])

    # Calculate average BLEU and ROUGE scores
    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)
    avg_rouge_score = sum(rouge_scores) / len(rouge_scores)

    print(f'Average BLEU score: {avg_bleu_score:.4f}')
    print(f'Average ROUGE-L score: {avg_rouge_score:.4f}')

    return avg_bleu_score, avg_rouge_score



def generate_sequence(model, device, tokenizer, context, vocab_size, max_length=256, context_size=256, top_k=10):
    """
    Generates a sequence using the provided model and tokenizer with the given context.

    Args:
        model: The trained transformer model.
        tokenizer: The tokenizer used to encode/decode tokens.
        context: The starting sequence to generate from (typically a list with the <SOS> token).
        vocab_size: Size of the vocabulary.
        max_length: The maximum length for the generated sequence.
        context_size: The maximum number of tokens the model can attend to.
        top_k: The number of top tokens to consider during sampling (for top-k sampling).

    Returns:
        generated_sequence: List of tokens generated by the model.
    """
    # Prepare the context (input sequence)
    context_tokens = tokenizer.tokenize(context)
    context_ids = torch.tensor([tokenizer.vocab[token] for token in context_tokens]).unsqueeze(0).to(device)
    generated_sequence = context_tokens[:-1]
    # print(generated_sequence)

    # Start generating tokens
    for _ in range(max_length):
        # print("Start a new token")
        # Get the logits for the next token from the model
        logits = model(context_ids)

        # Focus on the last token's logits (output of the transformer)
        logits = logits[:, -1, :]  # Shape: (batch_size, vocab_size)

        # Apply top-k sampling if top_k is set
        if top_k > 0:
            # Get the top-k token logits
            top_k_values, top_k_indices = torch.topk(logits, top_k)
            # Normalize probabilities
            top_k_probs = F.softmax(top_k_values, dim=-1)

            # Detach from the computation graph before using numpy()
            top_k_indices = top_k_indices.detach().cpu().numpy()[0]
            top_k_probs = top_k_probs.detach().cpu().numpy()[0]

            # Sample a token from the top-k candidates
            # print("Choices:")
            # print(top_k_indices)
            next_token_id = random.choices(top_k_indices, top_k_probs)[0]
        else:
            # Otherwise use argmax (greedy sampling)
            next_token_id = torch.argmax(logits, dim=-1).item()

        # Get the next token (from ID to token)
        # print(tokenizer.vocab)
        # print("Next token: ")
        # print(next_token_id)
        # print()
        next_token = list(tokenizer.vocab.keys())[list(tokenizer.vocab.values()).index(next_token_id)]

        # Add the generated token to the context sequence
        generated_sequence.append(next_token)

        # Append the next token to the context
        context_ids = torch.cat((context_ids, torch.tensor([[next_token_id]]).to(device)), dim=1)

        # Trim the context to the maximum context size
        if context_ids.size(1) > context_size:
            context_ids = context_ids[:, 1:]  # Drop the oldest token (shift left)

        # Stop if <EOS> token is generated
        if next_token == tokenizer.end_token:
            break

    # generated_sequence = tokenizer.detokenize(generated_sequence)
    # print(generated_sequence)
    print(generated_sequence)
    return generated_sequence


class TextDataset(Dataset):
    def __init__(self, tokenized_samples, vocab, pad_token_idx):
        self.tokenized_samples = tokenized_samples
        self.vocab = vocab
        self.pad_token_idx = pad_token_idx
        self.vocab_size = len(self.vocab)

    def __len__(self):
        return len(self.tokenized_samples)

    def __getitem__(self, idx):
        token_indices = torch.tensor([self.vocab[token] for token in self.tokenized_samples[idx]])
        return token_indices

def collate_fn(batch, pad_token_idx):
    inputs_padded = pad_sequence(batch, batch_first=True, padding_value=pad_token_idx)
    # Shift targets to the right
    targets_padded = inputs_padded.clone()
    targets_padded[:, :-1] = inputs_padded[:, 1:]
    targets_padded[:, -1] = pad_token_idx  # The last target token is padding
    return inputs_padded, targets_padded


def main():
    processor = ShakespeareDatasetProcessor('Shakespeare_data.csv')
    tokenizer = CharacterLevelTokenizer()
    processor.load_data()
    samples = processor.process_data()
    samples = [' '.join(sample) for sample in samples]

    processor.split_data(samples)
    train_tokens = [tokenizer.tokenize(sample) for sample in processor.train_samples]
    dev_tokens = [tokenizer.tokenize(sample) for sample in processor.dev_samples]
    eval_tokens = [tokenizer.tokenize(sample) for sample in processor.eval_samples]

    all_tokens = [char for tokens in (train_tokens + dev_tokens + eval_tokens) for char in tokens]
    vocab = {char: idx for idx, char in enumerate(set(all_tokens))}
    vocab['<PAD>'] = len(vocab)
    pad_token_idx = vocab['<PAD>']
    tokenizer.set_vocab(vocab)

    # Create the data loaders
    train_dataset = TextDataset(train_tokens, vocab, pad_token_idx)
    train_dataloader = DataLoader(train_dataset, batch_size=50, shuffle=True,
                                  collate_fn=lambda batch: collate_fn(batch, train_dataset.pad_token_idx))
    dev_dataset = TextDataset(dev_tokens, vocab, pad_token_idx)
    dev_dataloader = DataLoader(dev_dataset, batch_size=50, shuffle=False,
                                collate_fn=lambda batch: collate_fn(batch, train_dataset.pad_token_idx))
    eval_dataset = TextDataset(eval_tokens, vocab, pad_token_idx)
    eval_dataloader = DataLoader(eval_dataset, batch_size=50, shuffle=False,
                                 collate_fn=lambda batch: collate_fn(batch, train_dataset.pad_token_idx))

    vocab_size = len(train_dataset.vocab)
    model = DecoderOnlyTransformer(vocab_size=vocab_size, pad_token_idx=train_dataset.pad_token_idx, d_model=384, num_heads=8, d_ff=1536, num_layers=6)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    optimizer = optim.AdamW(model.parameters(), lr=2e-3, weight_decay=0.01)
    num_epochs = 25

    # Define scheduler
    num_training_steps = len(train_dataloader) * num_epochs
    num_warmup_steps = int(0.1 * num_training_steps)  # 10% warm-up
    scheduler = get_linear_schedule_with_warmup(optimizer,
                                                num_warmup_steps=num_warmup_steps,
                                                num_training_steps=num_training_steps)
    for epoch in range(num_epochs):
        # Training loop
        model.train()
        total_loss = 0.0
        for batch_idx, (inputs, targets) in tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f"Epoch {epoch+1}/{num_epochs} - Training"):
            inputs, targets = inputs.to(device), targets.to(device)
            logits = model(inputs)

            loss = causal_language_model_loss(logits, targets, train_dataset.pad_token_idx)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            scheduler.step()

            total_loss += loss.item()

        avg_train_loss = total_loss / len(train_dataloader)
        print(f"Epoch [{epoch+1}/{num_epochs}] - Training Loss: {avg_train_loss:.3f}")

        # Online Perplexity Calculation on the hold-out set
        model.eval()
        total_log_likelihood = 0.0
        total_token_count = 0
        total_loss = 0

        with torch.no_grad():
            for batch_idx, (inputs, targets) in tqdm(enumerate(dev_dataloader), total=len(dev_dataloader), desc=f"Epoch {epoch+1}/{num_epochs} - Dev PPL"):
                inputs, targets = inputs.to(device), targets.to(device)

                # Forward pass through the model
                logits = model(inputs)

                # Compute the causal loss (same as during training)
                loss = causal_language_model_loss(logits, targets, pad_token_idx=train_dataset.pad_token_idx)

                total_loss += loss.item()
                non_pad_tokens = (targets != train_dataset.pad_token_idx).sum().item()
                total_log_likelihood += loss.item() * non_pad_tokens  # Multiply by batch size
                total_token_count += non_pad_tokens

        # Calculate perplexity
        avg_log_likelihood = total_log_likelihood / total_token_count
        perplexity = torch.exp(torch.tensor(avg_log_likelihood))
        avg_dev_loss = total_loss / len(dev_dataloader)

        print(f"Epoch [{epoch+1}/{num_epochs}] - Dev Loss: {avg_dev_loss:.3f}")
        print(f"Epoch [{epoch+1}/{num_epochs}] - Dev Perplexity: {perplexity.item():.3f}")


    # Evaluate the model
    model.eval()
    total_log_likelihood = 0.0
    total_token_count = 0

    with torch.no_grad():
        for batch_idx, (inputs, targets) in tqdm(enumerate(eval_dataloader), total=len(eval_dataloader), desc="Test PPL"):
            inputs, targets = inputs.to(device), targets.to(device)
            logits = model(inputs)

            # Compute causal language model loss (similar to training)
            loss = causal_language_model_loss(logits, targets, pad_token_idx=train_dataset.pad_token_idx)

            non_pad_tokens = (targets != train_dataset.pad_token_idx).sum().item()
            total_log_likelihood += loss.item() * non_pad_tokens
            total_token_count += non_pad_tokens


    # Calculate final test perplexity
    avg_log_likelihood = total_log_likelihood / total_token_count
    test_perplexity = torch.exp(torch.tensor(avg_log_likelihood))
    print(f"Test Perplexity: {test_perplexity.item():.3f}")

    model.eval()

    # evaluate_bleu_and_rouge(model, tokenizer, device, processor.dev_samples, vocab_size)
    generate_sequence(model, device, tokenizer, "Richard:", vocab_size, max_length=40)

if __name__ == '__main__':
    main()